{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "## MachineLearning 第二课实训参考答案\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 简答题"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.同样是进行分类任务，LR与决策树在解决问题的思想上有哪些不同？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- LR是先构造线性函数，函数由特征和未知参数构成，然后对线性函数值进行sigmoid变换，变换成0-1的概率值，接近1的为一类，接近0的为一类。\n",
    "- 决策树是树状结构，自顶向下，每次选择一个特征进行分类，进而将样本分到不同的子节点。相比LR，决策树是非线性的，而且决策树还可以解决回归问题，在同一个子节点的用节点样本均值去预测，LR只能解决分类任务。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.如何理解决策树算法的可解释性很强这个优点？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:32:18.613753Z",
     "start_time": "2019-03-13T07:32:18.609088Z"
    }
   },
   "source": [
    "- 该算法是根据规则不断的将样本拆分，最终产生很多叶子节点，每个叶子节点对应的用户特征都是满足相同规则的。很容易了解样本的情况。\n",
    "- 决策树中树模型的结构是可以输出的,便于我们理解划分的规则,可以看到各个节点\n",
    "- 决策树是 if-then的形式规则判断来做划分便于理解"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.请解释决策树结构中的各个组成部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根结点、内部结点、叶子结点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.描述下树模型的生成过程是怎样的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "自顶向下不断分裂，每次分裂都选择最佳划分属性，直到树不能再分。\n",
    "\n",
    "- 最佳划分属性选择\n",
    " > 自根至叶结点递归，在每个中间结点寻找最优划分属性。划分属性的方式：信息增益（ID3算法）、信息增益率（C4.5算法)、基尼系数（CART算法）。\n",
    "- 树的停止条件\n",
    " >1）当前结点包含的样本都属于同一类，纯度很高，无需划分   \n",
    " >2）当前结点，样本属性集为空或者样本在属性上的取值相同，无法划分   \n",
    " >3）当前结点包含的样本集合为空或样本量很少，无法划分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.请列出熵的计算公式，并解释其意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Ent(D)=-\\sum_{i=0}^{n}p_ilog(p_i)$\n",
    "\n",
    "熵表示的是纯度，熵越大纯度越低，当结点中样本都属于某一类，熵的值为0，此时纯度最高。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6.请列出信息增益的计算公式，并解释其意义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "特征$A$对训练集$D$的信息增益\n",
    "\\begin{align*} \\\\ & g \\left( D, A \\right) = H \\left( D \\right) - H \\left( D | A \\right) \\end{align*}   \n",
    "即，集合$D$的经验熵$H \\left( D \\right)$与特征$A$给定条件下$D$的经验条件熵$H \\left( D | A \\right)$之差。 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.决策树是如何完成回归任务的？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "回归任务的损失函数是均方差，结点自顶向下分裂，每次分裂选择使得损失函数最小的属性及划分方式，分裂后样本分到不同的子节点中，一般用均值作为该结点的预测值（使得损失函数达到最小的预测值）。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.ID3,C4.5,CART三种树模型的区别在哪"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1、最佳属性选择方式不同   \n",
    "  >  ID3 根据信息增益，C4.5根据信息增益率，CART根据基尼稀疏\n",
    "\n",
    "2、处理的问题   \n",
    "  >  ID3、C4.5主要 是处理分类问题，CART被用来解决回归或者分类问题\n",
    "\n",
    "3、分支个数   \n",
    "  >  ID3、C4.5可以生成多个分支，CART是二分叉树\n",
    "\n",
    "4、特征使用次数   \n",
    "  >  对于连续特征，这三个模型都可以多次使用特征进行结点分裂，对于分类特征，只有CART模型可以多次使用该特征"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. 如何控制树模型的过拟合？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-03-13T07:27:50.976486Z",
     "start_time": "2019-03-13T07:27:50.970919Z"
    }
   },
   "source": [
    "1、增加样本量   \n",
    "2、控制模型复杂度，比如限制最大树深、限制最小叶子结点样本量、结点进行分裂的样本最小值   \n",
    "3、学习率   \n",
    "4、阈值限定，比如若信息增益小于某个值，停止增长   \n",
    "5、交叉验证，如果验证集上目标函数下降变缓慢或开始上升，则停止   \n",
    "6、剪枝"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10.连续变量与离散变量在树模型建模型时如何处理的 ？缺失值呢？"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 连续变量的处理\n",
    "> 将连续变量从左至右升序排列，然后从左至右扫描，每次选择相邻点的中点进行分割，将连续变量离散化\n",
    "- 离散变量的处理\n",
    "> 离散变量一般不需要处理，如果是在CART算法中，将离散变量onehotEncoder\n",
    "- 缺失值\n",
    "> 缺失值时修正属性分裂指标（如信息增益等），修正后分裂指标值=未缺失样本占比*未缺失样本的属性分裂指标值"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
