{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#4-Boosting提升方法\" data-toc-modified-id=\"4-Boosting提升方法-6\">4 Boosting提升方法</a></span><ul class=\"toc-item\"><li><span><a href=\"#4.1-提升方法（Boosting）\" data-toc-modified-id=\"4.1-提升方法（Boosting）-6.1\">4.1 提升方法（Boosting）</a></span></li><li><span><a href=\"#4.2-提升决策树-（BDT，Boosting-Decision-Tree）\" data-toc-modified-id=\"4.2-提升决策树-（BDT，Boosting-Decision-Tree）-6.2\">4.2 提升决策树 （BDT，Boosting Decision Tree）</a></span></li><li><span><a href=\"#4.3-梯度提升决策树-（GBDT，Gradient-Boosting-Decision-Tree）\" data-toc-modified-id=\"4.3-梯度提升决策树-（GBDT，Gradient-Boosting-Decision-Tree）-6.3\">4.3 梯度提升决策树 （GBDT，Gradient Boosting Decision Tree）</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Boosting提升方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 提升方法（Boosting）  \n",
    "&emsp;&emsp;提升方法使用加法模型和前向分步算法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp; &emsp;加法模型\n",
    "$$f\\left(x\\right)=\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.1}$$\n",
    "其中，$b\\left(x;\\gamma_m\\right)$为基函数，$\\gamma_m$为基函数的参数，$\\beta_m$为基函数的系数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;在给定训练数据$\\{\\left(x_i,y_i\\right)\\}_{i=1}^N$及损失函数$L\\left(y,f\\left(x\\right)\\right)$的条件下，学习加法模型$f\\left(x\\right)$成为经验风险极小化问题：\n",
    "$$\\min_{\\beta_m,\\gamma_m}\\sum_{i=1}^N L\\left(y_i,\\sum_{m=1}^M\\beta_m b\\left(x_i;\\gamma_m\\right)\\right)\\tag{1.2}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;前向分步算法求解这一优化问题的思路：因为学习的是加法模型，可以从前向后，每一步只学习一个基函数及其系数，逐步逼近优化目标函数式（1.2），则可以简化优化复杂度。具体地，每步只需优化如下损失函数：\n",
    "$$\\min_{\\beta,\\gamma}\\sum_{i=1}^N L\\left(y_i,\\beta b\\left(x_i;\\gamma\\right)\\right)\\tag{1.3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法1.1 前向分步算法  \n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$； 损失函数$L\\left(y,f\\left(x\\right)\\right)$；基函数集合$\\{b\\left(x;\\gamma\\right)\\}$；   \n",
    "输出：加法模型$f\\left(x\\right)$  \n",
    "（1）初始化$f_0\\left(x\\right)=0$  \n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "（a）极小化损失函数\n",
    "$$\\left(\\beta_m,\\gamma_m\\right)=\\mathop{\\arg\\min}_{\\beta,\\gamma} \\sum_{i=1}^N L\\left(y_i, f_{m-1}\\left(x_i\\right)+\\beta b\\left(x_i;\\gamma\\right)\\right) \\tag{1.4}$$  \n",
    "得到参数$\\beta_m$，$\\gamma_m$  \n",
    "（b）更新\n",
    "$$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.5}$$  \n",
    "（3）得到加法模型  \n",
    "$$f\\left(x\\right)=f_M\\left(x\\right)=\\sum_{m=1}^M\\beta_m b\\left(x;\\gamma_m\\right) \\tag{1.6}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;前向分步算法将同时求解从$m=1$到$M$所有参数$\\beta_m,\\gamma_m$的优化问题简化为逐次求解各个$\\beta_m, \\gamma_m$的优化问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 提升决策树 （BDT，Boosting Decision Tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;以决策树为基函数的提升方法为提升决策树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;提升决策树模型可以表示为决策树的加法模型：  \n",
    "$$f_M=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right) \\tag{2.1}$$  \n",
    "其中，$T\\left(x;\\Theta_m\\right)$表示决策树；$\\Theta_m$为决策树的参数；$M$为树的个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;提升决策树采用前向分步算法。首先确定初始提升决策树$f_0\\left(x\\right)=0$，第$m$步的模型是\n",
    "$$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right) \\tag{2.2}$$\n",
    "其中，$f_{m-1}\\left(x\\right)$为当前模型，通过经验风险极小化确定下一棵决策树的参数$\\Theta_m$，\n",
    "$$\\hat{\\Theta}_m=\\mathop{\\arg\\min}_{\\Theta_m}\\sum_{i=1}^N L\\left(y_i,f_{m-1}\\left(x_i\\right)+T\\left(x_i;\\Theta_m\\right)\\right) \\tag{2.3}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;已知训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots\\left(x_N,y_N\\right)\\}$，$x_i\\in\\mathcal{X}\\subseteq\\mathbb{R}^n$，$\\mathcal{X}$为输入空间，$y_i\\in\\mathcal{Y}\\subseteq\\mathbb{R}$，$\\mathcal{Y}$为输出空间。如果将输入空间$\\mathcal{X}$划分为$J$个互不相交的区域$R_1,R_2,\\dots,R_J$，并且在每个区域上确定输出的常量$c_j$，那么决策树可表示为\n",
    "$$T\\left(x;\\Theta\\right)=\\sum_{j=1}^J c_j I\\left(x\\in R_j\\right) \\tag{2.4}$$\n",
    "其中，参数$\\Theta=\\{\\left(R_1,c_1\\right),\\left(R_2,c_2\\right),\\dots,\\left(R_J,c_J\\right)\\}$表示决策树的区域划分和各区域上的常量值。$J$是决策树的复杂度即叶子结点个数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;提升决策树使用以下前向分步算法：\n",
    "$$\\begin{align}\n",
    "f_0\\left(x\\right)&=0 \\\\\n",
    "f_m\\left(x\\right)&=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right),\\quad m=1,2,\\dots,M        \\\\\n",
    "f_M\\left(x\\right)&=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right)\n",
    "\\end{align}$$  \n",
    "在前向分步算法的第$m$步，给定当前模型$f_{m-1}\\left(x\\right)$，需要求解  \n",
    "$$\\hat{\\Theta}_m=\\mathop{\\arg\\min}_{\\Theta_m}\\sum_{i=1}^N L\\left(y_i,f_{m-1}\\left(x_i\\right)+T\\left(x_i;\\Theta_m\\right)\\right)$$\n",
    "得到$\\hat{\\Theta}_m$，即第$m$棵树的参数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;当采用平方误差损失函数时，\n",
    "$$L\\left(y,f\\left(x\\right)\\right)=\\left(y-f\\left(x\\right)\\right)^2$$\n",
    "其损失变为\n",
    "$$\\begin{align}\n",
    "L\\left(y,f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right)\\right) \n",
    "&=\\left[y-f_{m-1}\\left(x\\right)-T\\left(x;\\Theta_m\\right)\\right]^2 \\\\\n",
    "&=\\left[r-T\\left(x;\\Theta_m\\right)\\right]^2\n",
    "\\end{align}$$\n",
    "其中，\n",
    "$$r=y-f_{m-1}\\left(x\\right) \\tag{2.5}$$\n",
    "是当前模型拟合数据的残差（residual）。对回归问题的提升决策树，只需要简单地拟合当前模型的残差。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法2.1 回归问题的提升决策树算法  \n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$；   \n",
    "输出：提升决策树$f_M\\left(x\\right)$  \n",
    "（1）初始化$f_0\\left(x\\right)=0$  \n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "（a）按照式（2.5）计算残差\n",
    "$$r_{mi}=y_i-f_{m-1}\\left(x_i\\right), \\quad i=1,2,\\dots,N$$  \n",
    "（b)拟合残差$r_{mi}$学习一个回归树，得到$T\\left(x;\\Theta_m\\right)$  \n",
    "（c）更新$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+T\\left(x;\\Theta_m\\right) $  \n",
    "（3）得到回归提升决策树 \n",
    "$$f_M\\left(x\\right)=\\sum_{m=1}^M T\\left(x;\\Theta_m\\right)   $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 梯度提升决策树 （GBDT，Gradient Boosting Decision Tree）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&emsp;&emsp;梯度提升算法使用损失函数的负梯度在当前模型的值\n",
    "$$-\\left[\\frac{\\partial L\\left(y,f\\left(x_i\\right)\\right)}{\\partial f\\left(x_i\\right)}\\right]_{f\\left(x\\right)=f_{m-1}\\left(x\\right)} \\tag{3.1}$$\n",
    "作为回归问题提升决策树算法中残差的近似值，拟合一个回归树。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "算法3.1 梯度提升算法  \n",
    "输入：训练数据集$T=\\{\\left(x_1,y_1\\right),\\left(x_2,y_2\\right),\\dots,\\left(x_N,y_N\\right)\\}$； 损失函数$L\\left(y,f\\left(x\\right)\\right)$  \n",
    "输出：梯度提升决策树$\\hat{f}\\left(x\\right)$  \n",
    "（1）初始化\n",
    "$$f_0\\left(x\\right)=\\mathop{\\arg\\min}_c\\sum_{i=1}^N L\\left(y_i,c\\right)$$\n",
    "（2）对$m=1,2,\\dots,M$  \n",
    "（a）对$i=1,2,\\dots,N$，计算\n",
    "$$r_{mi}=-\\left[\\frac{\\partial L\\left(y,f\\left(x_i\\right)\\right)}{\\partial f\\left(x_i\\right)}\\right]_{f\\left(x\\right)=f_{m-1}\\left(x\\right)}$$  \n",
    "（b)对$r_{mi}$拟合一个回归树，得到第$m$棵树的叶结点区域$R_{mj},j=1,2,\\dots,J$  \n",
    "（c）对$j=1,2,\\dots,J$，计算\n",
    "$$c_{mj}=\\mathop{\\arg\\min}_c\\sum_{x_i\\in R_{mj}} L\\left(y_i, f_{m-1}\\left(x_i\\right)+c\\right)$$  \n",
    "（d）更新$f_m\\left(x\\right)=f_{m-1}\\left(x\\right)+\\sum_{j=1}^J c_{mj} I\\left(x\\in R_{mj}\\right)$  \n",
    "（3）得到回归梯度提升决策树 \n",
    "$$\\hat{f}\\left(x\\right)=f_M\\left(x\\right)=\\sum_{m=1}^M \\sum_{j=1}^J c_{mj} I\\left(x\\in R_{mj}\\right)  $$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "toc": {
   "base_numbering": "6",
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
